{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b48815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from math import ceil\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import airbench\n",
    "import torch.linalg as LA\n",
    "from torch.cuda.amp import GradScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f7549cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Muon Optimizer\n",
    "@torch.compile\n",
    "def zeropower_via_newtonschulz5(G, steps=3, eps=1e-7):\n",
    "    assert len(G.shape) == 2\n",
    "    a, b, c = (3.4445, -4.7750,  2.0315)\n",
    "    X = G.bfloat16()\n",
    "    X /= (X.norm() + eps) # ensure top singular value <= 1\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.T\n",
    "        B = b * A + c * A @ A\n",
    "        X = a * X + B @ X\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    return X\n",
    "\n",
    "class Muon(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, momentum=0, nesterov=False):\n",
    "        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]\n",
    "            momentum = group[\"momentum\"]\n",
    "            for p in group[\"params\"]:\n",
    "                g = p.grad\n",
    "                if g is None:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "\n",
    "                if \"momentum_buffer\" not in state.keys():\n",
    "                    state[\"momentum_buffer\"] = torch.zeros_like(g)\n",
    "                buf = state[\"momentum_buffer\"]\n",
    "                buf.mul_(momentum).add_(g)\n",
    "                g = g.add(buf, alpha=momentum) if group[\"nesterov\"] else buf\n",
    "\n",
    "                p.data.mul_(len(p.data)**0.5 / p.data.norm()) # normalize the weight\n",
    "                update = zeropower_via_newtonschulz5(g.reshape(len(g), -1)).view(g.shape) # whiten the update\n",
    "                p.data.add_(update, alpha=-lr) # take a step\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "73122288",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL\n",
    "# note the use of low BatchNorm stats momentum\n",
    "class BatchNorm(nn.BatchNorm2d):\n",
    "    def __init__(self, num_features, momentum=0.6, eps=1e-12):\n",
    "        super().__init__(num_features, eps=eps, momentum=1-momentum)\n",
    "        self.weight.requires_grad = False\n",
    "        # Note that PyTorch already initializes the weights to one and bias to zero\n",
    "\n",
    "class Conv(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(in_channels, out_channels, kernel_size=3, padding=\"same\", bias=False)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()\n",
    "        w = self.weight.data\n",
    "        torch.nn.init.dirac_(w[:w.size(1)])\n",
    "\n",
    "class ConvGroup(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv(channels_in,  channels_out)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.norm1 = BatchNorm(channels_out)\n",
    "        self.conv2 = Conv(channels_out, channels_out)\n",
    "        self.norm2 = BatchNorm(channels_out)\n",
    "        self.activ = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.activ(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.activ(x)\n",
    "        return x\n",
    "\n",
    "class CifarNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        widths = dict(block1=64, block2=256, block3=256)\n",
    "        whiten_kernel_size = 2\n",
    "        whiten_width = 2 * 3 * whiten_kernel_size**2\n",
    "        self.whiten = nn.Conv2d(3, whiten_width, whiten_kernel_size, padding=0, bias=True)\n",
    "        self.whiten.weight.requires_grad = False\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.GELU(),\n",
    "            ConvGroup(whiten_width,     widths[\"block1\"]),\n",
    "            ConvGroup(widths[\"block1\"], widths[\"block2\"]),\n",
    "            ConvGroup(widths[\"block2\"], widths[\"block3\"]),\n",
    "            nn.MaxPool2d(3),\n",
    "        )\n",
    "        self.head = nn.Linear(widths[\"block3\"], 10, bias=False)\n",
    "        for mod in self.modules():\n",
    "            if isinstance(mod, BatchNorm):\n",
    "                mod.float()\n",
    "            else:\n",
    "                mod.half()\n",
    "\n",
    "    def reset(self):\n",
    "        for m in self.modules():\n",
    "            if type(m) in (nn.Conv2d, Conv, BatchNorm, nn.Linear):\n",
    "                m.reset_parameters()\n",
    "        w = self.head.weight.data\n",
    "        w *= 1 / w.std()\n",
    "\n",
    "    def init_whiten(self, train_images, eps=5e-4):\n",
    "        c, (h, w) = train_images.shape[1], self.whiten.weight.shape[2:]\n",
    "        patches = train_images.unfold(2,h,1).unfold(3,w,1).transpose(1,3).reshape(-1,c,h,w).float()\n",
    "        patches_flat = patches.view(len(patches), -1)\n",
    "        est_patch_covariance = (patches_flat.T @ patches_flat) / len(patches_flat)\n",
    "        eigenvalues, eigenvectors = torch.linalg.eigh(est_patch_covariance, UPLO=\"U\")\n",
    "        eigenvectors_scaled = eigenvectors.T.reshape(-1,c,h,w) / torch.sqrt(eigenvalues.view(-1,1,1,1) + eps)\n",
    "        self.whiten.weight.data[:] = torch.cat((eigenvectors_scaled, -eigenvectors_scaled))\n",
    "\n",
    "    def forward(self, x, whiten_bias_grad=True):\n",
    "        b = self.whiten.bias\n",
    "        x = F.conv2d(x, self.whiten.weight, b if whiten_bias_grad else b.detach())\n",
    "        x = self.layers(x)\n",
    "        x = x.view(len(x), -1)\n",
    "        return self.head(x) / x.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "adc523e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_effective_rank_matrix(W: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Safe SVD-based rank estimates.  Any NaN/Inf in W get zeroed out.\n",
    "    Returns (stable_rank, entropy_rank) as Python floats.\n",
    "    \"\"\"\n",
    "    # 1) Flatten + cast + to CPU\n",
    "    W2 = W.view(W.size(0), -1).float().cpu()\n",
    "    # 2) Zero out any non-finite entries\n",
    "    if not torch.isfinite(W2).all():\n",
    "        W2 = torch.nan_to_num(W2, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    # 3) Compute SVD on CPU\n",
    "    try:\n",
    "        _, s, _ = LA.svd(W2, full_matrices=False)\n",
    "    except RuntimeError as e:\n",
    "        # fallback: use eigenvalues of W W^T\n",
    "        # W W^T has the same non-zero singular values squared\n",
    "        cov = W2 @ W2.T\n",
    "        eig = torch.linalg.eigvalsh(cov)\n",
    "        s = torch.sqrt(torch.clamp(eig, min=0.0))\n",
    "\n",
    "    # stable rank = sum(s_i^2) / max(s_i)^2\n",
    "    stable = (s**2).sum() / (s.max()**2)\n",
    "    # entropy-based rank = exp(-sum p_i log p_i), p_i = s_i / sum(s_i)\n",
    "    p = s / s.sum()\n",
    "    entropy = torch.exp(- (p * torch.log(p)).sum())\n",
    "\n",
    "    return stable.item(), entropy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ef631db",
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_ranks = []\n",
    "entropy_ranks = []\n",
    "def train_muon():\n",
    "    model = CifarNet().cuda().to(memory_format=torch.channels_last)\n",
    "\n",
    "    batch_size = 2000\n",
    "    bias_lr = 0.053\n",
    "    head_lr = 0.67\n",
    "    wd = 2e-6 * batch_size\n",
    "\n",
    "    test_loader = airbench.CifarLoader(\"cifar10\", train=False, batch_size=2000)\n",
    "    train_loader = airbench.CifarLoader(\"cifar10\", train=True, batch_size=batch_size,\n",
    "                                        aug=dict(flip=True, translate=2), altflip=True)\n",
    "    total_train_steps = ceil(8 * len(train_loader))\n",
    "    whiten_bias_train_steps = ceil(3 * len(train_loader))\n",
    "\n",
    "    # Create optimizers and learning rate schedulers\n",
    "    filter_params = [p for p in model.parameters() if len(p.shape) == 4 and p.requires_grad]\n",
    "    norm_biases = [p for n, p in model.named_parameters() if \"norm\" in n and p.requires_grad]\n",
    "    param_configs = [dict(params=[model.whiten.bias], lr=bias_lr, weight_decay=wd/bias_lr),\n",
    "                     dict(params=norm_biases,         lr=bias_lr, weight_decay=wd/bias_lr),\n",
    "                     dict(params=[model.head.weight], lr=head_lr, weight_decay=wd/head_lr)]\n",
    "    optimizer1 = torch.optim.SGD(param_configs, momentum=0.85, nesterov=True, fused=True)\n",
    "    optimizer2 = Muon(filter_params, lr=0.24, momentum=0.6, nesterov=True)\n",
    "    optimizers = [optimizer1, optimizer2]\n",
    "    for opt in optimizers:\n",
    "        for group in opt.param_groups:\n",
    "            group[\"initial_lr\"] = group[\"lr\"]\n",
    "\n",
    "    model.reset()\n",
    "    step = 0\n",
    "\n",
    "    # Initialize the whitening layer using training images\n",
    "    train_images = train_loader.normalize(train_loader.images[:5000])\n",
    "    model.init_whiten(train_images)\n",
    "\n",
    "    for epoch in range(ceil(total_train_steps / len(train_loader))):\n",
    "\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs, whiten_bias_grad=(step < whiten_bias_train_steps))\n",
    "            F.cross_entropy(outputs, labels, label_smoothing=0.2, reduction=\"sum\").backward()\n",
    "            for group in optimizer1.param_groups[:1]:\n",
    "                group[\"lr\"] = group[\"initial_lr\"] * (1 - step / whiten_bias_train_steps)\n",
    "            for group in optimizer1.param_groups[1:]+optimizer2.param_groups:\n",
    "                group[\"lr\"] = group[\"initial_lr\"] * (1 - step / total_train_steps)\n",
    "            for opt in optimizers:\n",
    "                opt.step()\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            step += 1\n",
    "            if step >= total_train_steps:\n",
    "                break\n",
    "        \n",
    "        stable, entropy = compute_effective_rank_matrix(model.head.weight.data)\n",
    "        stable_ranks.append(stable)\n",
    "        entropy_ranks.append(entropy)\n",
    "\n",
    "\n",
    "    tta_val_acc = airbench.evaluate(model, test_loader, tta_level=2)\n",
    "    print(f\"{tta_val_acc:.4f}\")\n",
    "    return tta_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e547f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_ranks_adam = []\n",
    "entropy_ranks_adam = []\n",
    "\n",
    "def train_adam(\n",
    "    batch_size: int = 2000,\n",
    "    lr: float = 1e-3,\n",
    "    weight_decay: float = 2e-6,    # original per‚Äêparam decay\n",
    "    use_amp: bool = False,\n",
    "    device: str = \"cuda\",\n",
    "):\n",
    "    model = CifarNet().to(device).float().to(memory_format=torch.channels_last)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    scaler = GradScaler(enabled=use_amp)\n",
    "\n",
    "    # data\n",
    "    test_loader = airbench.CifarLoader(\"cifar10\", train=False, batch_size=2000)\n",
    "    train_loader = airbench.CifarLoader(\n",
    "        \"cifar10\", train=True, batch_size=batch_size,\n",
    "        aug=dict(flip=True, translate=2), altflip=True\n",
    "    )\n",
    "\n",
    "    total_steps       = ceil(8 * len(train_loader))\n",
    "    whiten_bias_steps = ceil(3 * len(train_loader))\n",
    "    num_epochs        = ceil(total_steps / len(train_loader))\n",
    "\n",
    "    # whiten init\n",
    "    model.reset()\n",
    "    train_imgs = train_loader.normalize(train_loader.images[:5000])\n",
    "    model.init_whiten(train_imgs)\n",
    "\n",
    "    step = 0\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device).float().to(memory_format=torch.channels_last)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    outputs = model(inputs, whiten_bias_grad=(step < whiten_bias_steps))\n",
    "                    loss    = F.cross_entropy(\n",
    "                        outputs, labels,\n",
    "                        label_smoothing=0.2,\n",
    "                        reduction=\"mean\"      # <-- mean, not sum\n",
    "                    )\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = model(inputs, whiten_bias_grad=(step < whiten_bias_steps))\n",
    "                loss    = F.cross_entropy(\n",
    "                    outputs, labels,\n",
    "                    label_smoothing=0.2,\n",
    "                    reduction=\"mean\"      # <-- mean\n",
    "                )\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            step += 1\n",
    "            if step >= total_steps:\n",
    "                break\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        # record ranks\n",
    "        stable, entropy = compute_effective_rank_matrix(model.head.weight.data)\n",
    "        stable_ranks_adam.append(stable)\n",
    "        entropy_ranks_adam.append(entropy)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch}/{num_epochs}  \"\n",
    "            f\"| loss = {avg_loss:.4f}  \"\n",
    "            f\"| stable = {stable:.3f}  \"\n",
    "            f\"| entropy = {entropy:.3f}\"\n",
    "        )\n",
    "\n",
    "    model.eval()\n",
    "    # tta_acc = airbench.evaluate(model, test_loader, tta_level=2)\n",
    "    # print(f\"\\nFinal TTA Accuracy (Adam): {tta_acc:.4f}\")\n",
    "    return model, stable_ranks_adam, entropy_ranks_adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "87fe9299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8348/1701925565.py:17: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8  | loss = 2.0103  | stable = 7.644  | entropy = 9.953\n",
      "Epoch 2/8  | loss = 1.7450  | stable = 7.523  | entropy = 9.952\n",
      "Epoch 3/8  | loss = 1.5928  | stable = 7.469  | entropy = 9.952\n",
      "Epoch 4/8  | loss = 1.4925  | stable = 7.448  | entropy = 9.951\n",
      "Epoch 5/8  | loss = 1.4221  | stable = 7.443  | entropy = 9.951\n",
      "Epoch 6/8  | loss = 1.3699  | stable = 7.448  | entropy = 9.952\n",
      "Epoch 7/8  | loss = 1.3298  | stable = 7.460  | entropy = 9.952\n",
      "Epoch 8/8  | loss = 1.2942  | stable = 7.475  | entropy = 9.952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8348/1701925565.py:85: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (c10::Half) and bias type (float) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_adam\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 86\u001b[39m, in \u001b[36mtrain_adam\u001b[39m\u001b[34m(batch_size, lr, weight_decay, use_amp, device)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Convert model inputs to same dtype as bias to avoid type mismatch\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.cuda.amp.autocast(enabled=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     tta_acc = \u001b[43mairbench\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtta_level\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFinal TTA Accuracy (Adam): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtta_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model, stable_ranks_adam, entropy_ranks_adam, tta_acc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/muon-blog/research/../airbench/utils.py:39\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(model, loader, tta_level)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate\u001b[39m(model, loader, tta_level=\u001b[32m0\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     logits = \u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtta_level\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (logits.argmax(\u001b[32m1\u001b[39m) == loader.labels).float().mean().item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/muon-blog/research/../airbench/utils.py:36\u001b[39m, in \u001b[36minfer\u001b[39m\u001b[34m(model, loader, tta_level)\u001b[39m\n\u001b[32m     34\u001b[39m infer_fn = [infer_basic, infer_mirror, infer_mirror_translate][tta_level]\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(\u001b[43m[\u001b[49m\u001b[43minfer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_images\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/muon-blog/research/../airbench/utils.py:36\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     34\u001b[39m infer_fn = [infer_basic, infer_mirror, infer_mirror_translate][tta_level]\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat([\u001b[43minfer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m inputs \u001b[38;5;129;01min\u001b[39;00m test_images.split(\u001b[32m2000\u001b[39m)])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/muon-blog/research/../airbench/utils.py:20\u001b[39m, in \u001b[36minfer.<locals>.infer_mirror_translate\u001b[39m\u001b[34m(inputs, net)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minfer_mirror_translate\u001b[39m(inputs, net):\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     logits = \u001b[43minfer_mirror\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     pad = \u001b[32m1\u001b[39m \n\u001b[32m     22\u001b[39m     padded_inputs = F.pad(inputs, (pad,)*\u001b[32m4\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mreflect\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/muon-blog/research/../airbench/utils.py:17\u001b[39m, in \u001b[36minfer.<locals>.infer_mirror\u001b[39m\u001b[34m(inputs, net)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minfer_mirror\u001b[39m(inputs, net):\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0.5\u001b[39m * \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m + \u001b[32m0.5\u001b[39m * net(inputs.flip(-\u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/muon-blog-qE-8R3YY-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/muon-blog-qE-8R3YY-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mCifarNet.forward\u001b[39m\u001b[34m(self, x, whiten_bias_grad)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, whiten_bias_grad=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m     77\u001b[39m     b = \u001b[38;5;28mself\u001b[39m.whiten.bias\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     x = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwhiten\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwhiten_bias_grad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.layers(x)\n\u001b[32m     80\u001b[39m     x = x.view(\u001b[38;5;28mlen\u001b[39m(x), -\u001b[32m1\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Input type (c10::Half) and bias type (float) should be the same"
     ]
    }
   ],
   "source": [
    "train_adam()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3797b1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8348/350201909.py:17: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (c10::Half) and bias type (float) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_adam\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mtrain_adam\u001b[39m\u001b[34m(batch_size, lr, weight_decay, use_amp, device)\u001b[39m\n",
      "\u001b[32m     53\u001b[39m     scaler.update()\n",
      "\u001b[32m     54\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhiten_bias_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhiten_bias_steps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     56\u001b[39m     loss    = F.cross_entropy(\n",
      "\u001b[32m     57\u001b[39m         outputs, labels,\n",
      "\u001b[32m     58\u001b[39m         label_smoothing=\u001b[32m0.2\u001b[39m,\n",
      "\u001b[32m     59\u001b[39m         reduction=\u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m      \u001b[38;5;66;03m# <-- mean\u001b[39;00m\n",
      "\u001b[32m     60\u001b[39m     )\n",
      "\u001b[32m     61\u001b[39m     loss.backward()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/muon-blog-qE-8R3YY-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/muon-blog-qE-8R3YY-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mCifarNet.forward\u001b[39m\u001b[34m(self, x, whiten_bias_grad)\u001b[39m\n",
      "\u001b[32m     76\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, whiten_bias_grad=\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[32m     77\u001b[39m     b = \u001b[38;5;28mself\u001b[39m.whiten.bias\n",
      "\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     x = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwhiten\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwhiten_bias_grad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     79\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.layers(x)\n",
      "\u001b[32m     80\u001b[39m     x = x.view(\u001b[38;5;28mlen\u001b[39m(x), -\u001b[32m1\u001b[39m)\n",
      "\n",
      "\u001b[31mRuntimeError\u001b[39m: Input type (c10::Half) and bias type (float) should be the same"
     ]
    }
   ],
   "source": [
    "train_adam()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf005a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = list(range(1, len(stable_ranks) + 1))\n",
    "plt.plot(epochs, stable_ranks, label=\"Stable rank\")\n",
    "plt.plot(epochs, entropy_ranks, label=\"Entropy‚Äêbased rank\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Effective rank\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (muon-blog)",
   "language": "python",
   "name": "muon-blog"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
